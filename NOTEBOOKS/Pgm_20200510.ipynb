{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validação Cruzada com Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atividade 6: Validacao_cruzada (k, dataset, ...) com Naive Bayes\n",
    "\n",
    "1) Dividir dataset em k partes estratificadas\n",
    "1.1) Determinar a quantidade de positivos, negativos, %pos e % neg de cada conjunto de folds (10 conjuntos)\n",
    "\n",
    "2) Definir um array de valores de c (α-alpha), iniciando com 0.1 até 1.0 (10 valores)\n",
    "\n",
    "3) Para Para cada valor de c (parâmetro (α-alpha) do classificador NB (NaiveBayes)):\n",
    "    3.1) Para cada parte i, de 1 até k (sendo o i o indice do fold que será considerado para teste\n",
    "    e os demais para o treinamento (j) sendo i ≠ j):\n",
    "    \n",
    "        3.1.1) Aplicar o classificador, com o c definido, sobre a parte i (predição) e calcular:\n",
    "            - Acurácia (Acur_NB[c][i])\n",
    "            - Revocação (Rev_BN[c][i])\n",
    "            - Precisão (Prec_NB[c][i])\n",
    "            \n",
    "    3.2) Calcular a média dos Rev_NB[i], Prec_NB[i] e Acur_NB[i], isto é, será a média dos indicadores para o conjunto\n",
    "    todo, que foi treinado e predito em partes.  \n",
    "\n",
    "4) Calcular o intervalo de confiança das médias de Rev_NB[c], Prec_NB[c] e Acur_NB[c]\n",
    "\n",
    "5) Analisar os resultados obtidos, observando os resultados para cada c (alpha) aplicado.\n",
    "\n",
    "6) Usar a Acurácia como balizador de escolha e identificar o parâmetro c (ótimo) de maior acurácia\n",
    "\n",
    "7) Retreinar o conjunto todo com o c ótimo\n",
    "\n",
    "8) Calcular o intervalo de confiança de Rev_NB[cotimo], Prec_NB[cotimo] e Acur_NB[cotimo]\n",
    "\n",
    "9) Aplicação dos itens 1 ao 8. As etapas de 1 a 8, acima, deverão ser aplicadas para:\n",
    "    - Todo o dataset (conjunto inteiros de características/componentes)\n",
    "    - O dataset resultante da aplicação do PCA (com somente as características calculadas retornadas pelo PCA)\n",
    "    - Os dataset´s resultantes dos dois selecionadores, com as características indicadas pelos selecionadores \n",
    "\n",
    "\n",
    "10) Apresentação:\n",
    "- Informações gerais do dataset (do que se trata a classificação binária, quantas instâncias, proporção de cada classe)\n",
    "\n",
    "- Métodos:\n",
    "\n",
    "    -- Tratamento realizado no dataset para uso com Naive Bayes (NB).\n",
    "    \n",
    "    -- Rebalanceamento utilizado (qual método)\n",
    "    \n",
    "    -- Pesquisa de missing values. Tratamento dos casos e método usado para correção.\n",
    "    \n",
    "    -- Aplicação de normalizalição. Qual.\n",
    "    \n",
    "    -- Implementação do NB. Bibliotecas utilizadas, adaptações.\n",
    "    \n",
    "    -- Implementou algum método, isto é, não utilizou frameworks prontos\n",
    "    \n",
    "    -- Como realizou a calibração. Parâmetros utilizados.\n",
    "\n",
    "- Resultados\n",
    "\n",
    "    -- Tabela Comparativa e gráficos\n",
    "\n",
    "- Discussão\n",
    "    \n",
    "    -- O que se esperava do treinamento e predições, variações no uso do alpha e no uso de validação cruzada\n",
    "\n",
    "    -- O conjunto de dados do dataset é adequado para este treinador (Naive Bayes)\n",
    "    \n",
    "    -- Discrepâncias e percepções dos resultados (% de acertos)esperado e obtido\n",
    "    \n",
    "    -- etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importações\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import statsmodels.stats.api as sms\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Leitura da Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bases a serem utilizadas - \n",
    "# 'df_final_20200510.csv' - 'df_kbest_fclassif_20200510.csv' - 'df_pca_features_importances_20200510.csv'\n",
    "# 'df_random_forest_importances_20200510.csv' - 'df_RFE_20200510.csv' - 'df_SBS_20200510.csv' - 'df_SFS_20200510.csv'\n",
    "\n",
    "df = pd.read_csv('df_RFE_20200510.csv')\n",
    "\n",
    "#print(\"Número de linhas e colunas:\", df.shape, '\\n')\n",
    "#print('\\n', df.info(), '\\n')\n",
    "\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Preparação do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparação e separação da base para as demais atividades\n",
    "dfx = df.copy()\n",
    "dfx.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Armazenando as classes\n",
    "classes = dfx['TARGET'].value_counts()\n",
    "attributes = list(df.columns)\n",
    "\n",
    "# Guardando a coluna Target em Y e removendo a coluna TARGET da base principal\n",
    "Y = dfx.loc[:, 'TARGET']\n",
    "dfx = dfx.drop('TARGET', axis=1)\n",
    "\n",
    "# Guardando a quantidade de linhas e colunas após a separação da coluna TARGET\n",
    "nrow, ncol = dfx.shape\n",
    "\n",
    "# Armazenando as colunas (atributos)\n",
    "attributes = list(dfx.columns)\n",
    "\n",
    "# Transformando para numpy\n",
    "X = dfx.to_numpy()\n",
    "\n",
    "# Normalizando\n",
    "scaler = StandardScaler().fit(X)\n",
    "X = scaler.transform(X)\n",
    "\n",
    "#dfx.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Partição do dataset em k folds\n",
    "<br> Como o dataset completo possui 217 linhas e 329 colunas, optamos em utilizar o SVD no lugar do PCA com \n",
    "dois componentes, já que no SVD não é possível trabalhar com todos os componentes.</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "#### Função de separação de uma base em k sub-conjuntos ####\n",
    "#### Recebe:                                            ####\n",
    "#### -- data: dataframe dos atributos                   ####\n",
    "#### -- target: array com a coluna alvo                 ####\n",
    "#### -- k: quantidade desejada de separação             ####\n",
    "####                                                    ####\n",
    "#### Retorna:                                           ####\n",
    "#### -- Os índices a serem considerados em cada parte   ####\n",
    "############################################################\n",
    "def splitFolds(data, target, k=10):\n",
    "    \n",
    "    # Contadores para apresentação\n",
    "    ldata = len(data)              # Quantidade de linhas da base\n",
    "    numel = int(ldata / k)         # Quantidade de amostras por fold\n",
    "        \n",
    "    uclass = target.value_counts() # Classes e suas quantidades\n",
    "    uclass.sort_index(inplace=True)\n",
    "    nclass = uclass.index          # Classes\n",
    "    qclass = uclass.values         # Quantidade de cada classe\n",
    "    \n",
    "    # Junção das classes e suas quantidades para um dicionário\n",
    "    zclass = zip(nclass, qclass)   \n",
    "    dclass = dict(zclass)          \n",
    "    \n",
    "    # Separação dos conjuntos\n",
    "    partesK = []    # Conterá todos os conjuntos k de índices, cada um proporcional a cada classe\n",
    "    for i in range(k):\n",
    "        pk = []\n",
    "        if (i < 7):\n",
    "            nelem = numel + 1\n",
    "        else:\n",
    "            nelem = numel\n",
    "            \n",
    "        ntot = nelem\n",
    "        \n",
    "        for nc, qc in dclass.items():\n",
    "            # Captura de todos os índices da coluna target\n",
    "            masc = target == nc\n",
    "            idclass = list(target[masc].index)\n",
    "        \n",
    "            # Montagem dos k subgrupos com a mesma proporção da classe\n",
    "            propclass = int(round(nelem * qc / ldata))\n",
    "            if (ntot > propclass):\n",
    "                ntot -= propclass\n",
    "            else:\n",
    "                propclass = ntot\n",
    "            \n",
    "            rs = random.sample(idclass, propclass)\n",
    "            pk = pk + rs\n",
    "            \n",
    "        partesK.append(pk)\n",
    "        \n",
    "    return (partesK)          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separação do dataset\n",
    "k = 10\n",
    "partesK = splitFolds(X, Y, k)\n",
    "\n",
    "# Estatísticas da separação\n",
    "qtdDS = len(X)\n",
    "vc = Y.value_counts()\n",
    "qtdT0 = vc.values[1]\n",
    "qtdT2 = vc.values[0]\n",
    "perT0 = round(qtdT0/qtdDS * 100, 2)\n",
    "perT2 = round(qtdT2/qtdDS * 100, 2)\n",
    "\n",
    "# Dicionário onde serão guardadas as informações do Dataset e suas execuções\n",
    "# chave ds - descrição geral de quantidades de classes e suas porcentagens\n",
    "dicSet = {}\n",
    "dicSet['ds'] = {}\n",
    "dicSet['ds']['qtdC0'] = qtdT0\n",
    "dicSet['ds']['qtdC2'] = qtdT2\n",
    "dicSet['ds']['perC0'] = perT0\n",
    "dicSet['ds']['perC2'] = perT2\n",
    "\n",
    "qtd0 = 0\n",
    "qtd2 = 0\n",
    "per0 = 0.0\n",
    "per2 = 0.0\n",
    "\n",
    "# chave folds - descrição de cada separação\n",
    "dicSet['ds']['fs'] = {}\n",
    "dicSet['ds']['fs']['k'] = k\n",
    "\n",
    "# Separação dos folds\n",
    "for f in range(len(partesK)):\n",
    "    dx = Y[partesK[f]]\n",
    "    vc = dx.value_counts()\n",
    "    qtd0 = vc.values[1]\n",
    "    qtd2 = vc.values[0]\n",
    "    per0 = round(qtd0/(qtd0 + qtd2) * 100, 2)\n",
    "    per2 = round(qtd2/(qtd0 + qtd2) * 100, 2)\n",
    "    \n",
    "    dicSet['ds']['fs'][f+1] = {}\n",
    "    dicSet['ds']['fs'][f+1]['qtdC0'] = qtd0\n",
    "    dicSet['ds']['fs'][f+1]['qtdC2'] = qtd2\n",
    "    dicSet['ds']['fs'][f+1]['perC0'] = per0\n",
    "    dicSet['ds']['fs'][f+1]['perC2'] = per2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ds': {'qtdC0': 101,\n",
       "  'qtdC2': 116,\n",
       "  'perC0': 46.54,\n",
       "  'perC2': 53.46,\n",
       "  'fs': {'k': 10,\n",
       "   1: {'qtdC0': 10, 'qtdC2': 12, 'perC0': 45.45, 'perC2': 54.55},\n",
       "   2: {'qtdC0': 10, 'qtdC2': 12, 'perC0': 45.45, 'perC2': 54.55},\n",
       "   3: {'qtdC0': 10, 'qtdC2': 12, 'perC0': 45.45, 'perC2': 54.55},\n",
       "   4: {'qtdC0': 10, 'qtdC2': 12, 'perC0': 45.45, 'perC2': 54.55},\n",
       "   5: {'qtdC0': 10, 'qtdC2': 12, 'perC0': 45.45, 'perC2': 54.55},\n",
       "   6: {'qtdC0': 10, 'qtdC2': 12, 'perC0': 45.45, 'perC2': 54.55},\n",
       "   7: {'qtdC0': 10, 'qtdC2': 12, 'perC0': 45.45, 'perC2': 54.55},\n",
       "   8: {'qtdC0': 10, 'qtdC2': 11, 'perC0': 47.62, 'perC2': 52.38},\n",
       "   9: {'qtdC0': 10, 'qtdC2': 11, 'perC0': 47.62, 'perC2': 52.38},\n",
       "   10: {'qtdC0': 10, 'qtdC2': 11, 'perC0': 47.62, 'perC2': 52.38}}}}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Aplicação do Naive Bayes - Bernouilli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicionário onde serão guardadas as informações do Dataset e suas execuções\n",
    "# - chave ds - descrição geral de quantidades de classes e suas porcentagens\n",
    "# - chave fs - descrição de cada separação - o fold de teste e para cada fold a quantidade e porcentagem\n",
    "# - chave NB - treinamento e aplicação do Naive Bayes, com:\n",
    "# -- Dicionário com as respostas dos treinamentos:\n",
    "# -- Por Parâmetro alpha\n",
    "# --- O Fold de teste considerado para predição (os demais são para treinamento)\n",
    "# ---- Quantidade de classes de predição\n",
    "# ---- Acurácia, Revocação, Precisão\n",
    "\n",
    "qtdRodadas = len(partesK)\n",
    "\n",
    "# Hiperparâmetro para treinamento - de 0.1 a 1.0\n",
    "alpha = list(np.arange(0.1, 1.1, 0.1))\n",
    "\n",
    "# Dicionário do Naive Bayes\n",
    "NB = {}  \n",
    "\n",
    "# Laço de repetição para todos alphas\n",
    "for al in alpha:    \n",
    "    # Cria uma entrada de chave no dicionário para cada alpha\n",
    "    NB[al] = {}\n",
    "    \n",
    "    # Laço de repetição para quantidade de folds\n",
    "    for ft in range(qtdRodadas):\n",
    "        # Cria uma entrada no dicionário (abaixo de alpha) para cada fold escolhido como teste\n",
    "        NB[al][ft] = {'classes':[], 'Acur':[], 'Rev':[], 'Prec':[]}\n",
    "    \n",
    "        # Montagem dos índices das partes de Teste e Treino\n",
    "        iTreino = []\n",
    "        iTeste = []\n",
    "        for i, p in enumerate(partesK):\n",
    "            if (i == ft):\n",
    "                # Fold das partesK que será utilizada para o teste\n",
    "                iTeste = p\n",
    "                continue\n",
    "        \n",
    "            # Montando os demais folds para Treinamento\n",
    "            for a in p:\n",
    "                iTreino.append(a)\n",
    "        \n",
    "        # Organiza os indices a serem considerados\n",
    "        iTreino.sort()\n",
    "        iTeste.sort()\n",
    "    \n",
    "        # Aplicando os índices para separar o dataset em conjunto de treinamento e teste\n",
    "        x_train, x_test = X[iTreino], X[iTeste]\n",
    "        y_train, y_test = Y[iTreino], Y[iTeste]\n",
    "        \n",
    "        # Treinando e aplicando o modelo NB\n",
    "        modelGNB = BernoulliNB(alpha = al)\n",
    "        modelGNB.fit(x_train, y_train)\n",
    "        y_pred = modelGNB.predict(x_test)\n",
    "    \n",
    "        # Cria um elemento no dicionário (abaixo de alpha e fold, guardando as quantidade de amostras por classe)\n",
    "        v, c = np.unique(y_pred, return_counts=True)\n",
    "        NB[al][ft]['classes'] = list(c)   \n",
    "            \n",
    "        # Calculando a matriz de confusão\n",
    "        mat = list(confusion_matrix(y_test, y_pred))\n",
    "        vp = mat[0][0]\n",
    "        vn = mat[0][1]\n",
    "        fp = mat[1][0]\n",
    "        fn = mat[1][1]\n",
    "        # Acurácia = VP + VN)/n\n",
    "        NB[al][ft]['Acur'] = (vp + vn) / len(y_pred)\n",
    "        # Revocação = VP / (VP + FN))\n",
    "        NB[al][ft]['Rev'] = vp / (vp + fn)\n",
    "        # Precisão (VP / (VP + FP))\n",
    "        NB[al][ft]['Prec'] = vp / (vp + fp)\n",
    "                \n",
    "# Dicionário avalia, utilizado para os treinamentos e predições será incorporado ao dicionário dicSet\n",
    "dicSet['ds']['NB'] = NB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ds': {'qtdC0': 101,\n",
       "  'qtdC2': 116,\n",
       "  'perC0': 46.54,\n",
       "  'perC2': 53.46,\n",
       "  'fs': {'k': 10,\n",
       "   1: {'qtdC0': 10, 'qtdC2': 12, 'perC0': 45.45, 'perC2': 54.55},\n",
       "   2: {'qtdC0': 10, 'qtdC2': 12, 'perC0': 45.45, 'perC2': 54.55},\n",
       "   3: {'qtdC0': 10, 'qtdC2': 12, 'perC0': 45.45, 'perC2': 54.55},\n",
       "   4: {'qtdC0': 10, 'qtdC2': 12, 'perC0': 45.45, 'perC2': 54.55},\n",
       "   5: {'qtdC0': 10, 'qtdC2': 12, 'perC0': 45.45, 'perC2': 54.55},\n",
       "   6: {'qtdC0': 10, 'qtdC2': 12, 'perC0': 45.45, 'perC2': 54.55},\n",
       "   7: {'qtdC0': 10, 'qtdC2': 12, 'perC0': 45.45, 'perC2': 54.55},\n",
       "   8: {'qtdC0': 10, 'qtdC2': 11, 'perC0': 47.62, 'perC2': 52.38},\n",
       "   9: {'qtdC0': 10, 'qtdC2': 11, 'perC0': 47.62, 'perC2': 52.38},\n",
       "   10: {'qtdC0': 10, 'qtdC2': 11, 'perC0': 47.62, 'perC2': 52.38}},\n",
       "  'NB': {0.1: {0: {'classes': [10, 12],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.4375,\n",
       "     'Prec': 0.7},\n",
       "    1: {'classes': [13, 9],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.5384615384615384,\n",
       "     'Prec': 0.5384615384615384},\n",
       "    2: {'classes': [10, 12],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.42857142857142855,\n",
       "     'Prec': 0.6},\n",
       "    3: {'classes': [11, 11],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.46153846153846156,\n",
       "     'Prec': 0.5454545454545454},\n",
       "    4: {'classes': [10, 12],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.4375,\n",
       "     'Prec': 0.7},\n",
       "    5: {'classes': [12, 10],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.5,\n",
       "     'Prec': 0.6666666666666666},\n",
       "    6: {'classes': [13, 9],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.5263157894736842,\n",
       "     'Prec': 0.7692307692307693},\n",
       "    7: {'classes': [12, 9],\n",
       "     'Acur': 0.47619047619047616,\n",
       "     'Rev': 0.5333333333333333,\n",
       "     'Prec': 0.6666666666666666},\n",
       "    8: {'classes': [11, 10],\n",
       "     'Acur': 0.47619047619047616,\n",
       "     'Rev': 0.5,\n",
       "     'Prec': 0.8181818181818182},\n",
       "    9: {'classes': [13, 8],\n",
       "     'Acur': 0.47619047619047616,\n",
       "     'Rev': 0.5833333333333334,\n",
       "     'Prec': 0.5384615384615384}},\n",
       "   0.2: {0: {'classes': [10, 12],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.4375,\n",
       "     'Prec': 0.7},\n",
       "    1: {'classes': [13, 9],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.5384615384615384,\n",
       "     'Prec': 0.5384615384615384},\n",
       "    2: {'classes': [10, 12],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.42857142857142855,\n",
       "     'Prec': 0.6},\n",
       "    3: {'classes': [11, 11],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.46153846153846156,\n",
       "     'Prec': 0.5454545454545454},\n",
       "    4: {'classes': [10, 12],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.4375,\n",
       "     'Prec': 0.7},\n",
       "    5: {'classes': [12, 10],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.5,\n",
       "     'Prec': 0.6666666666666666},\n",
       "    6: {'classes': [13, 9],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.5263157894736842,\n",
       "     'Prec': 0.7692307692307693},\n",
       "    7: {'classes': [12, 9],\n",
       "     'Acur': 0.47619047619047616,\n",
       "     'Rev': 0.5333333333333333,\n",
       "     'Prec': 0.6666666666666666},\n",
       "    8: {'classes': [11, 10],\n",
       "     'Acur': 0.47619047619047616,\n",
       "     'Rev': 0.5,\n",
       "     'Prec': 0.8181818181818182},\n",
       "    9: {'classes': [13, 8],\n",
       "     'Acur': 0.47619047619047616,\n",
       "     'Rev': 0.5833333333333334,\n",
       "     'Prec': 0.5384615384615384}},\n",
       "   0.30000000000000004: {0: {'classes': [10, 12],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.4375,\n",
       "     'Prec': 0.7},\n",
       "    1: {'classes': [14, 8],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.5833333333333334,\n",
       "     'Prec': 0.5},\n",
       "    2: {'classes': [10, 12],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.42857142857142855,\n",
       "     'Prec': 0.6},\n",
       "    3: {'classes': [11, 11],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.46153846153846156,\n",
       "     'Prec': 0.5454545454545454},\n",
       "    4: {'classes': [10, 12],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.4375,\n",
       "     'Prec': 0.7},\n",
       "    5: {'classes': [12, 10],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.5,\n",
       "     'Prec': 0.6666666666666666},\n",
       "    6: {'classes': [13, 9],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.5263157894736842,\n",
       "     'Prec': 0.7692307692307693},\n",
       "    7: {'classes': [12, 9],\n",
       "     'Acur': 0.47619047619047616,\n",
       "     'Rev': 0.5333333333333333,\n",
       "     'Prec': 0.6666666666666666},\n",
       "    8: {'classes': [11, 10],\n",
       "     'Acur': 0.47619047619047616,\n",
       "     'Rev': 0.5,\n",
       "     'Prec': 0.8181818181818182},\n",
       "    9: {'classes': [13, 8],\n",
       "     'Acur': 0.47619047619047616,\n",
       "     'Rev': 0.5833333333333334,\n",
       "     'Prec': 0.5384615384615384}},\n",
       "   0.4: {0: {'classes': [10, 12],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.4375,\n",
       "     'Prec': 0.7},\n",
       "    1: {'classes': [15, 7],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.6153846153846154,\n",
       "     'Prec': 0.5333333333333333},\n",
       "    2: {'classes': [10, 12],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.42857142857142855,\n",
       "     'Prec': 0.6},\n",
       "    3: {'classes': [11, 11],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.46153846153846156,\n",
       "     'Prec': 0.5454545454545454},\n",
       "    4: {'classes': [10, 12],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.4375,\n",
       "     'Prec': 0.7},\n",
       "    5: {'classes': [12, 10],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.5,\n",
       "     'Prec': 0.6666666666666666},\n",
       "    6: {'classes': [13, 9],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.5263157894736842,\n",
       "     'Prec': 0.7692307692307693},\n",
       "    7: {'classes': [12, 9],\n",
       "     'Acur': 0.47619047619047616,\n",
       "     'Rev': 0.5333333333333333,\n",
       "     'Prec': 0.6666666666666666},\n",
       "    8: {'classes': [11, 10],\n",
       "     'Acur': 0.47619047619047616,\n",
       "     'Rev': 0.5,\n",
       "     'Prec': 0.8181818181818182},\n",
       "    9: {'classes': [13, 8],\n",
       "     'Acur': 0.47619047619047616,\n",
       "     'Rev': 0.5833333333333334,\n",
       "     'Prec': 0.5384615384615384}},\n",
       "   0.5: {0: {'classes': [10, 12],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.4375,\n",
       "     'Prec': 0.7},\n",
       "    1: {'classes': [15, 7],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.6153846153846154,\n",
       "     'Prec': 0.5333333333333333},\n",
       "    2: {'classes': [10, 12],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.42857142857142855,\n",
       "     'Prec': 0.6},\n",
       "    3: {'classes': [11, 11],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.46153846153846156,\n",
       "     'Prec': 0.5454545454545454},\n",
       "    4: {'classes': [10, 12],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.4375,\n",
       "     'Prec': 0.7},\n",
       "    5: {'classes': [12, 10],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.5,\n",
       "     'Prec': 0.6666666666666666},\n",
       "    6: {'classes': [13, 9],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.5263157894736842,\n",
       "     'Prec': 0.7692307692307693},\n",
       "    7: {'classes': [12, 9],\n",
       "     'Acur': 0.47619047619047616,\n",
       "     'Rev': 0.5333333333333333,\n",
       "     'Prec': 0.6666666666666666},\n",
       "    8: {'classes': [11, 10],\n",
       "     'Acur': 0.47619047619047616,\n",
       "     'Rev': 0.5,\n",
       "     'Prec': 0.8181818181818182},\n",
       "    9: {'classes': [13, 8],\n",
       "     'Acur': 0.47619047619047616,\n",
       "     'Rev': 0.5833333333333334,\n",
       "     'Prec': 0.5384615384615384}},\n",
       "   0.6: {0: {'classes': [10, 12],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.4375,\n",
       "     'Prec': 0.7},\n",
       "    1: {'classes': [15, 7],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.6153846153846154,\n",
       "     'Prec': 0.5333333333333333},\n",
       "    2: {'classes': [10, 12],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.42857142857142855,\n",
       "     'Prec': 0.6},\n",
       "    3: {'classes': [11, 11],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.46153846153846156,\n",
       "     'Prec': 0.5454545454545454},\n",
       "    4: {'classes': [10, 12],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.4375,\n",
       "     'Prec': 0.7},\n",
       "    5: {'classes': [12, 10],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.5,\n",
       "     'Prec': 0.6666666666666666},\n",
       "    6: {'classes': [13, 9],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.5263157894736842,\n",
       "     'Prec': 0.7692307692307693},\n",
       "    7: {'classes': [12, 9],\n",
       "     'Acur': 0.47619047619047616,\n",
       "     'Rev': 0.5333333333333333,\n",
       "     'Prec': 0.6666666666666666},\n",
       "    8: {'classes': [11, 10],\n",
       "     'Acur': 0.47619047619047616,\n",
       "     'Rev': 0.5,\n",
       "     'Prec': 0.8181818181818182},\n",
       "    9: {'classes': [13, 8],\n",
       "     'Acur': 0.47619047619047616,\n",
       "     'Rev': 0.5833333333333334,\n",
       "     'Prec': 0.5384615384615384}},\n",
       "   0.7000000000000001: {0: {'classes': [10, 12],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.4375,\n",
       "     'Prec': 0.7},\n",
       "    1: {'classes': [15, 7],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.6153846153846154,\n",
       "     'Prec': 0.5333333333333333},\n",
       "    2: {'classes': [10, 12],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.42857142857142855,\n",
       "     'Prec': 0.6},\n",
       "    3: {'classes': [11, 11],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.46153846153846156,\n",
       "     'Prec': 0.5454545454545454},\n",
       "    4: {'classes': [10, 12],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.4375,\n",
       "     'Prec': 0.7},\n",
       "    5: {'classes': [12, 10],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.5,\n",
       "     'Prec': 0.6666666666666666},\n",
       "    6: {'classes': [13, 9],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.5263157894736842,\n",
       "     'Prec': 0.7692307692307693},\n",
       "    7: {'classes': [12, 9],\n",
       "     'Acur': 0.47619047619047616,\n",
       "     'Rev': 0.5333333333333333,\n",
       "     'Prec': 0.6666666666666666},\n",
       "    8: {'classes': [11, 10],\n",
       "     'Acur': 0.47619047619047616,\n",
       "     'Rev': 0.5,\n",
       "     'Prec': 0.8181818181818182},\n",
       "    9: {'classes': [13, 8],\n",
       "     'Acur': 0.47619047619047616,\n",
       "     'Rev': 0.5833333333333334,\n",
       "     'Prec': 0.5384615384615384}},\n",
       "   0.8: {0: {'classes': [10, 12],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.4375,\n",
       "     'Prec': 0.7},\n",
       "    1: {'classes': [15, 7],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.6153846153846154,\n",
       "     'Prec': 0.5333333333333333},\n",
       "    2: {'classes': [10, 12],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.42857142857142855,\n",
       "     'Prec': 0.6},\n",
       "    3: {'classes': [11, 11],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.46153846153846156,\n",
       "     'Prec': 0.5454545454545454},\n",
       "    4: {'classes': [10, 12],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.4375,\n",
       "     'Prec': 0.7},\n",
       "    5: {'classes': [12, 10],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.5,\n",
       "     'Prec': 0.6666666666666666},\n",
       "    6: {'classes': [13, 9],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.5263157894736842,\n",
       "     'Prec': 0.7692307692307693},\n",
       "    7: {'classes': [12, 9],\n",
       "     'Acur': 0.47619047619047616,\n",
       "     'Rev': 0.5333333333333333,\n",
       "     'Prec': 0.6666666666666666},\n",
       "    8: {'classes': [11, 10],\n",
       "     'Acur': 0.47619047619047616,\n",
       "     'Rev': 0.5,\n",
       "     'Prec': 0.8181818181818182},\n",
       "    9: {'classes': [13, 8],\n",
       "     'Acur': 0.47619047619047616,\n",
       "     'Rev': 0.5833333333333334,\n",
       "     'Prec': 0.5384615384615384}},\n",
       "   0.9: {0: {'classes': [10, 12],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.4375,\n",
       "     'Prec': 0.7},\n",
       "    1: {'classes': [15, 7],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.6153846153846154,\n",
       "     'Prec': 0.5333333333333333},\n",
       "    2: {'classes': [10, 12],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.42857142857142855,\n",
       "     'Prec': 0.6},\n",
       "    3: {'classes': [11, 11],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.46153846153846156,\n",
       "     'Prec': 0.5454545454545454},\n",
       "    4: {'classes': [10, 12],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.4375,\n",
       "     'Prec': 0.7},\n",
       "    5: {'classes': [12, 10],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.5,\n",
       "     'Prec': 0.6666666666666666},\n",
       "    6: {'classes': [13, 9],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.5263157894736842,\n",
       "     'Prec': 0.7692307692307693},\n",
       "    7: {'classes': [12, 9],\n",
       "     'Acur': 0.47619047619047616,\n",
       "     'Rev': 0.5333333333333333,\n",
       "     'Prec': 0.6666666666666666},\n",
       "    8: {'classes': [11, 10],\n",
       "     'Acur': 0.47619047619047616,\n",
       "     'Rev': 0.5,\n",
       "     'Prec': 0.8181818181818182},\n",
       "    9: {'classes': [13, 8],\n",
       "     'Acur': 0.47619047619047616,\n",
       "     'Rev': 0.5833333333333334,\n",
       "     'Prec': 0.5384615384615384}},\n",
       "   1.0: {0: {'classes': [10, 12],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.4375,\n",
       "     'Prec': 0.7},\n",
       "    1: {'classes': [15, 7],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.6153846153846154,\n",
       "     'Prec': 0.5333333333333333},\n",
       "    2: {'classes': [10, 12],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.42857142857142855,\n",
       "     'Prec': 0.6},\n",
       "    3: {'classes': [11, 11],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.46153846153846156,\n",
       "     'Prec': 0.5454545454545454},\n",
       "    4: {'classes': [10, 12],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.4375,\n",
       "     'Prec': 0.7},\n",
       "    5: {'classes': [12, 10],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.5,\n",
       "     'Prec': 0.6666666666666666},\n",
       "    6: {'classes': [13, 9],\n",
       "     'Acur': 0.45454545454545453,\n",
       "     'Rev': 0.5263157894736842,\n",
       "     'Prec': 0.7692307692307693},\n",
       "    7: {'classes': [12, 9],\n",
       "     'Acur': 0.47619047619047616,\n",
       "     'Rev': 0.5333333333333333,\n",
       "     'Prec': 0.6666666666666666},\n",
       "    8: {'classes': [11, 10],\n",
       "     'Acur': 0.47619047619047616,\n",
       "     'Rev': 0.5,\n",
       "     'Prec': 0.8181818181818182},\n",
       "    9: {'classes': [13, 8],\n",
       "     'Acur': 0.47619047619047616,\n",
       "     'Rev': 0.5833333333333334,\n",
       "     'Prec': 0.5384615384615384}}}}}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculando as médias dos kfolds por parâmetro alpha\n",
    "nb = dicSet['ds']['NB']\n",
    "\n",
    "# Quantidade de kfolds\n",
    "n = dicSet['ds']['fs']['k']\n",
    "\n",
    "# Guardando num dicionário de médias\n",
    "dicMedia = {}\n",
    "\n",
    "# Por cada alpha - ka\n",
    "for ka, va in nb.items():\n",
    "    dicMedia[ka] = {}\n",
    "    \n",
    "    # Por cada fold acumula a acurácia, precisão e revocação e determina a média de cada\n",
    "    ttclasses = [0.00, 0.00]\n",
    "    tacu = 0.00\n",
    "    trev = 0.00\n",
    "    tpre = 0.00\n",
    "    lacu = []\n",
    "    lrev = []\n",
    "    lpre = []\n",
    "    for kf, vf in va.items():\n",
    "        ttclasses[0] += vf['classes'][0]\n",
    "        ttclasses[1] += vf['classes'][1]\n",
    "        tacu += vf['Acur']\n",
    "        trev += vf['Rev']\n",
    "        tpre += vf['Prec']\n",
    "        \n",
    "        lacu.append(vf['Acur'])\n",
    "        lrev.append(vf['Rev'])\n",
    "        lpre.append(vf['Prec'])\n",
    "\n",
    "    # Calculando a média\n",
    "    dicMedia[ka]['QtClasses'] = ttclasses\n",
    "    dicMedia[ka]['M_Acu'] = tacu / n\n",
    "    dicMedia[ka]['M_Rev'] = trev / n\n",
    "    dicMedia[ka]['M_Pre'] = tpre / n\n",
    "\n",
    "    # Calculo do Intervalo de Confiança\n",
    "    dicMedia[ka]['I_Acu'] = sms.DescrStatsW(lacu).tconfint_mean(alpha=0.05)\n",
    "    dicMedia[ka]['I_Rev'] = sms.DescrStatsW(lrev).tconfint_mean(alpha=0.05)\n",
    "    dicMedia[ka]['I_Pre'] = sms.DescrStatsW(lpre).tconfint_mean(alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.1: {'QtClasses': [115.0, 102.0],\n",
       "  'M_Acu': 0.461038961038961,\n",
       "  'M_Rev': 0.49465538847117785,\n",
       "  'M_Pre': 0.6543123543123543,\n",
       "  'I_Acu': (0.453559519168725, 0.46851840290919705),\n",
       "  'I_Rev': (0.45745717458010376, 0.531853602362252),\n",
       "  'I_Pre': (0.5843104173103604, 0.7243142913143481)},\n",
       " 0.2: {'QtClasses': [115.0, 102.0],\n",
       "  'M_Acu': 0.461038961038961,\n",
       "  'M_Rev': 0.49465538847117785,\n",
       "  'M_Pre': 0.6543123543123543,\n",
       "  'I_Acu': (0.453559519168725, 0.46851840290919705),\n",
       "  'I_Rev': (0.45745717458010376, 0.531853602362252),\n",
       "  'I_Pre': (0.5843104173103604, 0.7243142913143481)},\n",
       " 0.30000000000000004: {'QtClasses': [116.0, 101.0],\n",
       "  'M_Acu': 0.461038961038961,\n",
       "  'M_Rev': 0.4991425679583574,\n",
       "  'M_Pre': 0.6504662004662005,\n",
       "  'I_Acu': (0.453559519168725, 0.46851840290919705),\n",
       "  'I_Rev': (0.45778707389216, 0.540498062024555),\n",
       "  'I_Pre': (0.5764210730464657, 0.7245113278859351)},\n",
       " 0.4: {'QtClasses': [117.0, 100.0],\n",
       "  'M_Acu': 0.461038961038961,\n",
       "  'M_Rev': 0.5023476961634856,\n",
       "  'M_Pre': 0.6537995337995339,\n",
       "  'I_Acu': (0.453559519168725, 0.46851840290919705),\n",
       "  'I_Rev': (0.4568536466276613, 0.5478417456993099),\n",
       "  'I_Pre': (0.5833071356514106, 0.7242919319476568)},\n",
       " 0.5: {'QtClasses': [117.0, 100.0],\n",
       "  'M_Acu': 0.461038961038961,\n",
       "  'M_Rev': 0.5023476961634856,\n",
       "  'M_Pre': 0.6537995337995339,\n",
       "  'I_Acu': (0.453559519168725, 0.46851840290919705),\n",
       "  'I_Rev': (0.4568536466276613, 0.5478417456993099),\n",
       "  'I_Pre': (0.5833071356514106, 0.7242919319476568)},\n",
       " 0.6: {'QtClasses': [117.0, 100.0],\n",
       "  'M_Acu': 0.461038961038961,\n",
       "  'M_Rev': 0.5023476961634856,\n",
       "  'M_Pre': 0.6537995337995339,\n",
       "  'I_Acu': (0.453559519168725, 0.46851840290919705),\n",
       "  'I_Rev': (0.4568536466276613, 0.5478417456993099),\n",
       "  'I_Pre': (0.5833071356514106, 0.7242919319476568)},\n",
       " 0.7000000000000001: {'QtClasses': [117.0, 100.0],\n",
       "  'M_Acu': 0.461038961038961,\n",
       "  'M_Rev': 0.5023476961634856,\n",
       "  'M_Pre': 0.6537995337995339,\n",
       "  'I_Acu': (0.453559519168725, 0.46851840290919705),\n",
       "  'I_Rev': (0.4568536466276613, 0.5478417456993099),\n",
       "  'I_Pre': (0.5833071356514106, 0.7242919319476568)},\n",
       " 0.8: {'QtClasses': [117.0, 100.0],\n",
       "  'M_Acu': 0.461038961038961,\n",
       "  'M_Rev': 0.5023476961634856,\n",
       "  'M_Pre': 0.6537995337995339,\n",
       "  'I_Acu': (0.453559519168725, 0.46851840290919705),\n",
       "  'I_Rev': (0.4568536466276613, 0.5478417456993099),\n",
       "  'I_Pre': (0.5833071356514106, 0.7242919319476568)},\n",
       " 0.9: {'QtClasses': [117.0, 100.0],\n",
       "  'M_Acu': 0.461038961038961,\n",
       "  'M_Rev': 0.5023476961634856,\n",
       "  'M_Pre': 0.6537995337995339,\n",
       "  'I_Acu': (0.453559519168725, 0.46851840290919705),\n",
       "  'I_Rev': (0.4568536466276613, 0.5478417456993099),\n",
       "  'I_Pre': (0.5833071356514106, 0.7242919319476568)},\n",
       " 1.0: {'QtClasses': [117.0, 100.0],\n",
       "  'M_Acu': 0.461038961038961,\n",
       "  'M_Rev': 0.5023476961634856,\n",
       "  'M_Pre': 0.6537995337995339,\n",
       "  'I_Acu': (0.453559519168725, 0.46851840290919705),\n",
       "  'I_Rev': (0.4568536466276613, 0.5478417456993099),\n",
       "  'I_Pre': (0.5833071356514106, 0.7242919319476568)}}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicMedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gravando uma planilha em excel\n",
    "saida = pd.DataFrame(dicMedia)\n",
    "saida.to_csv('Arq_NB_01_20200513.csv') #, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
